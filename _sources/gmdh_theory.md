# Теория МГУА

Метод группового учёта аргументов (МГУА) - это метод машинного обучения, разработанный в 1968 году советским академиком Алексеем Григорьевичем Ивахненко. МГУА стал одним из первых в истории алгоритмов глубокого обучения.

## Основные принципы

### 1. Самоорганизация

Большинство методов машинного обучения устроено так, что в процессе обучения алгоритмов просто подбираются численные параметры для определённой конкретной модели. Под *моделью* здесь подразумевается какая-либо функция, которая описывает зависимость целевой величины от входных данных. 

:::{admonition} Example
Например, в машинном обучении есть алгоритм линейной регрессии, который использует одну фиксированную модель вида

$$
y=w_0+\sum_{i=1}^nw_ix_i
$$

где $y$ - целевая величина, $(x_1, ...,x_n)$ - вектор входных данных, $(w_0,w_1,...,w_n)$ - параметры модели. И задача в таком случае сводится лишь к подбору таких параметров модели, которые бы давали наиболее точные прогнозы значения $y$. 
:::

Главной же особенностью, на которой основан метод группового учёта аргументов, является *самоорганизация*. Данный принцип заключается в том, что в процессе обучения алгоритм не только подбирает численные параметры, но и самостоятельно определяет наиболее подходящую структуру модели. Вместо того, чтобы перед началом обучения ограничивать алгоритм одной моделью, МГУА задаёт целый класс моделей. Далее в процессе обучения из этого класса перебираются различные модели-кандидаты и среди них выбирается одна оптимальная. Возможности проверки сразу множества отличающихся своей структурой моделей и автоматического определения лучшей из них кардинально отличают МГУА от прочих методов машинного обучения.

### 2. Способность избегать переобучения

Ещё одной принципиально важной характеристикой МГУА является его *способность избегать переобучения*. Переобучение - серьёзная проблема, с которой сталикаются многие алгоритмы машинного обучения. Суть её заключается в том, что модель может показывать отличные результаты на тренировочных данных, но при этом на тестовой выборке точность оказывается очень низкой. Обычно это происходит из-за чрезмерного усложнения модели. МГУА решает данную проблему благодаря особенному способу перебора моделей. 

На первой итерации выбираются и обучаются самые простые модели-кандидаты из заданного класса моделей. Затем на каждом следующем этапе сложность рассматриваемых моделей постепенно возрастает и этот процесс продолжается лишь до тех пор, пока точность результатов растёт. Итерации в МГУА называются *рядами*, и каждый ряд соответствует перебору моделей-кандидатов одинаковой сложности. Как только на очередном ряду обнаруживается, что качество моделей перестало улучшаться, процесс обучения тут же завершается. Обосновывается данный подход тем, что простые модели плохо отражают искомые закономерности, тогда как сложные модели чувствительны к шумам в данных. Таким образом, МГУА автоматически отдаёт предпочтение моделям с оптимальными точностью и сложностью, тем самым минимизируя вероятность переобучения.

### 3. Применение критериев

Каким образом МГУА обучает модели-кандидаты и ищет среди них оптимальную? Для этих целей используются *внутренние и внешние критерии*. Внутренний критерий отвечает за подбор лучших числовых параметров в рамках одной модели. Роль внутреннего критерия выполняет метод наименьших квадратов (МНК), который основан на минимизации суммы квадратов отклонений некоторой функции от целевых данных. Другими словами, методами линейной алгебры подбираются такие параметры модели, при которых достигается минимально возможное значение функции среднеквадратичной ошибки (MSE)

$$
MSE=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2
$$

где $y_i$ - истинное значение $i$-й переменной, $\hat{y_i}$ - предсказанное моделью значение $i$-й переменной, $n$ - количество переменных.

Внешний критерий представляет собой определённую формулу, которая использует данные о параметрах обученной модели и сделанных ею прогнозах на новых данных для вычисления количественной меры точности этой модели. Чем меньше значение внешнего критерия, тем лучше модель. Внешний критерий является гиперпараметром алгоритма и задаётся до этапа обучения. В главе [Внешние критерии](../criterions.md) представлена классификация и подробное описание различных внешних критериев.

(subsamples)=
### 4. Разбиение данных на три подвыборки

Важной концепцией алгоритма МГУА также является *разбиение данных на три подвыборки*. В отличие от классического принципа деления датасета на тренировочную и тестовую части, МГУА выделяет из исходных данных следующие компоненты:
- обучающая выборка:
    - тренировочная часть;
    - тестовая часть;
- проверочная выборка.

В процессе обучения участвуют сразу тренировочная и тестовая части. На тренировочной части данных применяется внутренний критерий и подбираются параметры моделей. На тестовой части данных происходит построение прогнозов и отбор лучших моделей по внешнему критерию. Проверочная часть применяется уже после окончания обучения и необходима для объективной оценки качества найденной оптимальной модели.

:::{note}
Некоторые внешние критерии подразумевают иное использование тренировочной и тестовой частей выборки. Подробнее об этом написано в разделе [внешние критерии](../criterions.md).
:::

(hyperparameters)=
## Гиперпараметры МГУА

Гиперпараметры - это параметры алгоритма машинного обучения, настройка которых позволяет управлять процессом обучения. Гиперпараметры задаются вручную до этапа обучения. 

Общими гиперпараметрами алгоритмов МГУА являются:
- [внешний критерий](../criterions.md) (`criterion`);
- доля тестовой части в обучающей выборке (`test_size`);
- количество лучших моделей ряда, на основе которых будет рассчитано качество всего ряда (`p_average`);
- минимально необходимое улучшение качества нового ряда для принятия решения о продолжении обучения (`limit`).

Кроме того, некоторые алгоритмы имеют дополнительные гиперпараметры:
- количество лучших моделей, на основе которых будут формироваться усложнённые модели на следующем ряду (`k_best`);
- {ref}`вид базовых полиномов<polynomials>`, из которых будут формироваться модели разного уровня сложности (`polynomial_type`).

Гиперпараметры и их возможные значения подробно описаны в разделах с обзором конкретных алгоритмов МГУА.

## Обобщённый алгоритм обучения

1. Разделение обучающей выборки на тренировочную и тестовые части в соответствии с гиперпараметром `test_size`.
2. Генерация моделей-кандидатов для текущего ряда.
3. Расчёт параметров для всех моделей-кандидатов текущего ряда методом МНК.
4. Оценка всех моделей-кандидатов текущего ряда по выбранному внешнему критерию `criterion`.
5. Выбор `p_average` лучших моделей-кандидатов текущего ряда.
6. Расчёт ошибки ряда как среднего значения внешних критериев лучших моделей текущего ряда.
7. Если ошибка текущего ряда меньше ошибки предыдущего ряда более, чем на `limit`, или предыдущий ряд отсутствует, переход на ряд следующего уровня сложности моделей и возврат к пункту 2. Иначе лучшая модель текущего ряда принимается за модель оптимальной сложности и обучение останавливается.

Далее при использовании обученного алгоритма все прогнозы будет совершаться одной оптимальной моделью.

## МГУА и нейронные сети

Нейронные сети, как и МГУА, являются алгоритмами глубокого обучения. Они извлекают знания из входных наборов данных и аппроксимируют сложные закономерности, найденные в них. Однако подходы, заложенные в МГУА и нейронные сети, сильно отличаются. Описанные выше принципы работы МГУА частично устраняют некоторые распространённые проблемы, с которыми приходится сталкиваться при работе с нейронными сетями.

Для настройки нейронной сети необходимо самостоятельно выбрать определённую архитектуру: число слоёв, количество нейронов, вид активационной функции и так далее. Все эти параметры каждый раз подбираются методом проб и ошибок с учётом знаний теории нейронных сетей и априорной информации об исследуемой предметной области. При этом всегда присутствует шанс, что модель может недообучиться или переобучиться. А при использовании метода группового учёта аргументов достаточно лишь ввыбрать подходящий алгоритм МГУА и указать соответствующий цели решаемой задачи внешний критерий. В процессе обучения алгоритм индуктивным методом сам подберёт подходящую внутреннюю структуру.

Нейронная сеть для поиска оптимальных параметров модели использует численные методы. Например, метод обратного распространения ошибки. Это требует определённого времени на многократные пересчёты параметров, а результат обучения имеет зависимость от начальной инициализации параметров. Алгоритмы МГУА не нуждаются в случайной инициализации параметров, так как вычисляет параметры моделей аналитически, решая задачу наименьших квадратов с использованием, например, QR-разложения матрицы.

Найденное нейронной сетью решение далеко не всегда является глобальным минимумом функции потерь, тогда как МГУА из заданного класса моделей-кандидатов всегда находит одну модель оптимальной сложности.

Для качественного обучения нейросети обычно требуются большие наборы данных, в то время как МГУА способен подбирать оптимальные модели даже для экстремально коротких и зашумлённых выборок. 

Таким образом, метод группового учёта аргументов, имея ряд преимуществ над нейронными сетями, является эффективным и актуальным методом решения задач аппроксимации и прогнозирования. Но даже с учётом того, что МГУА берёт на себя задачу не только параметрической, но и структурной параметризации, максимум пользы от его использования можно получить, применяя наиболее подходящую для конкретной задачи разновидность алгоритма с настройкой соответствующих ей гиперпараметров.